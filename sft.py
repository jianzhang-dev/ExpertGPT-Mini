import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import os
import json
import random
import time
from tqdm import tqdm
from transformers import BertTokenizer
from torch.utils.data import Dataset, DataLoader
# ====================ã€æ–°å¢ï¼šä½™å¼¦é€€ç«è°ƒåº¦å™¨ã€‘====================
class CosineLRScheduler:
    @staticmethod
    def get_cosine_schedule(optimizer, num_warmup_steps, num_training_steps, min_lr_ratio=0.1):
        def lr_lambda(current_step):
            if current_step < num_warmup_steps:
                return float(current_step) / float(max(1, num_warmup_steps))
            progress = float(current_step - num_warmup_steps) / float(max(1, num_training_steps - num_warmup_steps))
            return max(min_lr_ratio, 0.5 * (1.0 + math.cos(math.pi * progress)))
        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
# ====================ã€æ–°å¢ç»“æŸã€‘====================
# é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªè¾…åŠ©å‡½æ•°æ¥åˆ›å»ºå› æœæ»‘åŠ¨çª—å£æ©ç 
def create_causal_sliding_window_mask(seq_len, window_size):
    causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
    dists = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len).unsqueeze(0)
    sliding_window_mask = (dists >= 0) & (dists < window_size)
    return sliding_window_mask
class SingleAttentionExpert(nn.Module):
    def __init__(self, hidden_size, num_heads, window_size):
        super().__init__()
        assert hidden_size % num_heads == 0
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.window_size = window_size
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
    def forward(self, x, attention_mask=None, rope=None):
        batch_size, seq_len, _ = x.shape
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        if rope is not None:
            q = rope(q)
            k = rope(k)
        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask, is_causal=False)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        return self.o_proj(attn_output)
class ParallelExpertAttention(nn.Module):
    def __init__(self, hidden_size=384, num_experts=4, num_heads=6, window_size=16, is_global=False):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.window_size = window_size
        self.is_global = is_global  # ã€æ–°å¢ã€‘ä¿å­˜æ˜¯å¦ä¸ºå…¨å±€æ³¨æ„åŠ›å±‚
        
        self.experts = nn.ModuleList(
            [SingleAttentionExpert(hidden_size, num_heads, window_size) for _ in range(num_experts)]
        )
        fused_dim = num_experts * hidden_size
        self.gate_proj = nn.Linear(fused_dim, hidden_size, bias=False)
        self.up_proj = nn.Linear(fused_dim, hidden_size, bias=False)
        self.dropout = nn.Dropout(0.1)
        self.register_buffer("sliding_window_mask", None, persistent=False)
        self.last_mask_len = 0

    def _get_mask(self, seq_len, device):
        if self.last_mask_len != seq_len or self.sliding_window_mask is None:
            if self.is_global:
                # ã€æ–°å¢ã€‘å…¨å±€æ¨¡å¼ï¼šæ ‡å‡†çš„ä¸‹ä¸‰è§’å› æœæ©ç  (Full Causal Mask)
                # å…è®¸çœ‹åˆ°æ‰€æœ‰ä¹‹å‰çš„ tokenï¼Œæ‰“ç ´ window_size é™åˆ¶
                mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
            else:
                # ã€åŸæœ‰ã€‘å±€éƒ¨æ¨¡å¼ï¼šæ»‘åŠ¨çª—å£æ©ç 
                mask = create_causal_sliding_window_mask(seq_len, self.window_size)
            
            self.sliding_window_mask = mask.to(device)
            self.last_mask_len = seq_len
        return self.sliding_window_mask

    def forward(self, x, rope=None):
        batch_size, seq_len, _ = x.shape
        attention_mask = self._get_mask(seq_len, x.device)
        expert_outputs = []
        for expert in self.experts:
            output = expert(x, attention_mask=attention_mask, rope=rope)
            expert_outputs.append(output)
        fused_output = torch.cat(expert_outputs, dim=-1)
        gate = F.silu(self.gate_proj(fused_output))
        value = self.up_proj(fused_output)
        fused_result = self.dropout(gate * value)
        return fused_result
class GeGLUFeedForward(nn.Module):
    def __init__(self, hidden_size, expansion_ratio=3):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = int(hidden_size * expansion_ratio)
        self.gate_proj = nn.Linear(hidden_size, self.intermediate_size)
        self.up_proj = nn.Linear(hidden_size, self.intermediate_size)
        self.down_proj = nn.Linear(self.intermediate_size, hidden_size)
        self.dropout = nn.Dropout(0.1)
    def forward(self, x):
        x = F.silu(self.gate_proj(x)) * self.up_proj(x)
        x = self.down_proj(x)
        x = self.dropout(x)
        return x
class ExpertTransformerLayer(nn.Module):
    def __init__(self, hidden_size=384, num_experts=4, num_heads=6, window_size=16, is_global=False):
        super().__init__()
        # ã€ä¿®æ”¹ã€‘å°† is_global å‚æ•°ä¼ é€’ç»™ ParallelExpertAttention
        self.self_attn = ParallelExpertAttention(
            hidden_size=hidden_size,
            num_experts=num_experts,
            num_heads=num_heads,
            window_size=window_size,
            is_global=is_global  # ä¼ é€’æ ‡å¿—ä½
        )
        self.ffn = GeGLUFeedForward(hidden_size)
        self.norm1 = nn.RMSNorm(hidden_size)
        self.norm2 = nn.RMSNorm(hidden_size)

    def forward(self, x, rope=None):
        attn_output = self.self_attn(self.norm1(x), rope=rope)
        x = x + attn_output
        ffn_output = self.ffn(self.norm2(x))
        x = x + ffn_output
        return x
class RotaryPositionEmbedding(nn.Module):
    def __init__(self, dim, num_heads, max_seq_len=256, base_min=2000.0, base_max=100000.0):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.max_seq_len = max_seq_len
        self.current_seq_len_cached = 0
        num_head_groups = num_heads // 2
        group_bases = torch.logspace(
            start=math.log10(base_min),
            end=math.log10(base_max),
            steps=num_head_groups
        )
        base_list = [base for base in group_bases for _ in range(2)]
        self.base_list = torch.tensor(base_list)
        self.register_buffer("base_cache", self.base_list, persistent=True)
        self._update_freqs(seq_len=max_seq_len)
    def _update_freqs(self, seq_len, device='cpu'):
        if seq_len > self.max_seq_len:
            alpha = (seq_len / self.max_seq_len)
            current_bases = self.base_cache * (alpha ** (self.dim / (self.dim - 2)))
        else:
            current_bases = self.base_cache
        current_bases = current_bases.to(device)
        inv_freq_list = [1.0 / (base ** (torch.arange(0, self.dim, 2, device=device).float() / self.dim)) for base in current_bases]
        inv_freq = torch.stack(inv_freq_list)
        t = torch.arange(seq_len, device=device, dtype=inv_freq.dtype)
        freqs = torch.einsum('i,hj->hij', t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer('cos_cached', emb.cos(), persistent=False)
        self.register_buffer('sin_cached', emb.sin(), persistent=False)
        self.current_seq_len_cached = seq_len
    def forward(self, x, seq_len=None):
        if seq_len is None:
            seq_len = x.shape[-2]
        if self.cos_cached.device != x.device or seq_len > self.current_seq_len_cached:
            self._update_freqs(seq_len, device=x.device)
        cos = self.cos_cached[:, :seq_len, ...].unsqueeze(0)
        sin = self.sin_cached[:, :seq_len, ...].unsqueeze(0)
        x1, x2 = x[..., : self.dim // 2], x[..., self.dim // 2 :]
        rotated = torch.cat((-x2, x1), dim=-1)
        return (x * cos) + (rotated * sin)
class ExpertGPTModel(nn.Module):
    def __init__(self, vocab_size=21128, hidden_size=384, num_layers=4,
                 num_experts=4, num_heads=6, window_size=16, max_seq_len=256):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.max_seq_len = max_seq_len
        self.num_experts = num_experts
        self.window_size = window_size
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        head_dim = hidden_size // num_heads
        self.rope = RotaryPositionEmbedding(head_dim, num_heads, max_seq_len=max_seq_len)
        
        # ã€ä¿®æ”¹ã€‘è®¡ç®—å…¨å±€æ³¨æ„åŠ›å±‚çš„ç´¢å¼• (1/3 å’Œ 2/3 å¤„)
        # ä½¿ç”¨é›†åˆå¤„ç†ï¼Œé˜²æ­¢å±‚æ•°æå°‘æ—¶ç´¢å¼•é‡å¤
        global_layer_indices = {num_layers // 3, (num_layers * 2) // 3}
        
        # ã€ä¿®æ”¹ã€‘åŠ¨æ€æ„å»ºå±‚åˆ—è¡¨ï¼Œä¼ å…¥ is_global å‚æ•°
        layers = []
        for i in range(num_layers):
            is_global = i in global_layer_indices
            if is_global:
                print(f"  -> Layer {i}: è®¾ç½®ä¸ºå…¨å±€æ³¨æ„åŠ›å±‚ (Global Attention)")
            
            layers.append(
                ExpertTransformerLayer(
                    hidden_size=hidden_size,
                    num_experts=num_experts,
                    num_heads=num_heads,
                    window_size=window_size,
                    is_global=is_global  # ä¼ å…¥å½“å‰å±‚æ˜¯å¦ä¸ºå…¨å±€çš„æ ‡å¿—
                )
            )
        self.layers = nn.ModuleList(layers)

        self.final_norm = nn.RMSNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        x = self.token_embedding(input_ids)
        for layer in self.layers:
            x = layer(x, rope=self.rope)
        x = self.final_norm(x)
        logits = self.lm_head(x)
        return logits

    def generate(self, input_ids, max_length=50, temperature=0.8, top_p=0.9, repetition_penalty=1.2):
        self.eval()
        generated = input_ids
        appeared_tokens = set(generated[0].tolist())
        with torch.no_grad():
            for _ in range(max_length):
                logits = self.forward(generated)[:, -1, :]
                if repetition_penalty != 1.0:
                    for token in appeared_tokens:
                        logits[0, token] /= repetition_penalty
                if temperature > 0:
                    logits = logits / temperature
                    probs = F.softmax(logits, dim=-1)
                    if top_p < 1.0:
                        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0
                        indices_to_remove = sorted_indices[sorted_indices_to_remove]
                        probs[..., indices_to_remove] = 0
                        if probs.sum() > 0:
                            probs = probs / probs.sum()
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)
                next_token_item = next_token.item()
                appeared_tokens.add(next_token_item)
                generated = torch.cat([generated, next_token], dim=1)
                if next_token.item() in [102, 0]:
                    break
        self.train()
        return generated
# --- å¤ç”¨æ‚¨ä»£ç åº“ä¸­çš„æ•°æ®å¤„ç†å’Œè®­ç»ƒç»„ä»¶ ---
class OpenSourceTokenizer:
    def __init__(self):
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        self.vocab_size = self.tokenizer.vocab_size
        print(f"âœ… ä½¿ç”¨å¼€æºåˆ†è¯å™¨ï¼Œè¯æ±‡è¡¨å¤§å°: {self.vocab_size}")
    def encode(self, text):
        return self.tokenizer.encode(text, add_special_tokens=False)
    def decode(self, ids):
        return self.tokenizer.decode(ids, skip_special_tokens=True)
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length=256):
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length
    def __len__(self):
        return len(self.texts)
    def __getitem__(self, idx):
        text = self.texts[idx]
        tokens = self.tokenizer.encode(text)
        tokens = tokens + [102]
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]
        else:
            tokens = tokens + [0] * (self.max_length - len(tokens))
        input_ids = torch.tensor(tokens, dtype=torch.long)
        return {'input_ids': input_ids, 'labels': input_ids.clone()}

# ====================ã€æ›¿æ¢ä¸ºå¾®è°ƒæ•°æ®é›†åŠ è½½é€»è¾‘ã€‘====================
class DataManager:
    def __init__(self):
        # æ·»åŠ åˆå§‹åŒ– tokenizer
        self.tokenizer = OpenSourceTokenizer()
        self.max_length = 700
        
    def load_datasets(self):
        """åŠ è½½è®­ç»ƒæ•°æ®é›† - åŒ…æ‹¬æ–°å¢çš„ 7.json"""
        print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
        all_texts = []
        
        # 1. åŠ è½½ Why é—®ç­”æ•°æ®
        why_path = "data/raw/why.json"  
        if os.path.exists(why_path):
            print("åŠ è½½Whyé—®ç­”æ•°æ®...")
            why_texts = self._load_why_data(why_path)
            all_texts.extend(why_texts)
            print(f"Whyæ•°æ®: {len(why_texts)} æ¡")
        
        # 2. åŠ è½½ Alpaca æ•°æ®
        alpaca_path = "data/raw/alpaca_gpt4_data_zh.json"
        if os.path.exists(alpaca_path):
            print("åŠ è½½AlpacaæŒ‡ä»¤æ•°æ®...")
            alpaca_texts = self._load_alpaca_data(alpaca_path)
            all_texts.extend(alpaca_texts)
            print(f"Alpacaæ•°æ®: {len(alpaca_texts)} æ¡")
        
        # 3. åŠ è½½ Firefly æ•°æ®
        firefly_path = "data/raw/firefly-train-1.1M.jsonl"
        if os.path.exists(firefly_path):
            print("åŠ è½½Fireflyæ•°æ®...")
            firefly_texts = self._load_firefly_data(firefly_path)
            all_texts.extend(firefly_texts)
            print(f"Fireflyæ•°æ®: {len(firefly_texts)} æ¡")
        
        # 4. ã€æ–°å¢ã€‘åŠ è½½ 7.jsonï¼ˆä¸ä¸Šè¿°æ•°æ®é›†åŒè·¯å¾„ï¼‰
        json7_path = "data/raw/7.json"
        if os.path.exists(json7_path):
            print("åŠ è½½7.jsonæ•°æ®...")
            json7_texts = self._load_json7_data(json7_path)
            all_texts.extend(json7_texts)
            print(f"7.jsonæ•°æ®: {len(json7_texts)} æ¡")
        
        # 5. å¤‡ç”¨æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼‰
        if not all_texts:
            print("ä½¿ç”¨è‡ªåŠ¨ç”Ÿæˆçš„å¤‡ç”¨æ•°æ®...")
            all_texts = self._generate_backup_data()
        else:
            print(f"æ€»å…±åŠ è½½äº† {len(all_texts)} æ¡æ–‡æœ¬")
        
        # æ•°æ®æ¸…æ´—
        cleaned_texts = self._clean_texts(all_texts)
        print(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œæœ‰æ•ˆæ–‡æœ¬: {len(cleaned_texts)} æ¡")
        
        # æ‰€æœ‰æ•°æ®éƒ½ä½œä¸ºè®­ç»ƒé›†
        train_texts = cleaned_texts
        print(f"è®­ç»ƒé›†: {len(train_texts)} æ¡")
        return train_texts, []

    def _load_alpaca_data(self, alpaca_path):
        """åŠ è½½Alpacaæ•°æ®"""
        alpaca_texts = []
        try:
            with open(alpaca_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            for item in tqdm(data, desc="å¤„ç†Alpacaæ•°æ®"):
                instruction = item.get('instruction', '')
                input_text = item.get('input', '')
                output_text = item.get('output', '')
                if instruction and output_text:
                    if input_text and input_text.strip():
                        text = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nå›ç­”ï¼š{output_text}"
                    else:
                        text = f"æŒ‡ä»¤ï¼š{instruction}\nå›ç­”ï¼š{output_text}"
                    if len(text) > 3:
                        alpaca_texts.append(text)
        except Exception as e:
            print(f"Alpacaæ•°æ®åŠ è½½é”™è¯¯: {e}")
        return alpaca_texts

    def _load_firefly_data(self, firefly_path):
        """åŠ è½½Fireflyæ•°æ®ï¼Œè¿‡æ»¤æŒ‡å®šç±»åˆ«"""
        firefly_texts = []
        filtered_categories = ["MusicComment", "ClassicalChinese", "Cot", "Translation","ProductDesc"]
        kept_count = 0
        filtered_count = 0
        try:
            with open(firefly_path, 'r', encoding='utf-8') as f:
                for line in tqdm(f, desc="å¤„ç†Fireflyæ•°æ®"):
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        item = json.loads(line)
                        input_text = item.get('input', '')
                        target_text = item.get('target', '')
                        kind = item.get('kind', '')
                        # è¿‡æ»¤æŒ‡å®šç±»åˆ«
                        if kind in filtered_categories:
                            filtered_count += 1
                            continue
                        if input_text and target_text:
                            # Fireflyæ•°æ®æ ¼å¼è½¬æ¢ä¸ºæŒ‡ä»¤æ ¼å¼
                            text = f"æŒ‡ä»¤ï¼šå®Œæˆä»¥ä¸‹ä»»åŠ¡\nè¾“å…¥ï¼š{input_text}\nå›ç­”ï¼š{target_text}"
                            if len(text) > 4:
                                firefly_texts.append(text)
                                kept_count += 1
                    except json.JSONDecodeError:
                        continue
            print(f"Fireflyæ•°æ®: ä¿ç•™ {kept_count} æ¡ï¼Œè¿‡æ»¤ {filtered_count} æ¡")
        except Exception as e:
            print(f"Fireflyæ•°æ®åŠ è½½é”™è¯¯: {e}")
        return firefly_texts

    def _load_why_data(self, why_path):
        """åŠ è½½Whyé—®ç­”æ•°æ® - æµå¼è¯»å–å¤§æ–‡ä»¶"""
        why_texts = []
        try:
            print(f"æ­£åœ¨æµå¼è¯»å–å¤§æ–‡ä»¶: {why_path}")
            # ä½¿ç”¨ijsonæµå¼è§£æå¤§JSONæ–‡ä»¶
            try:
                import ijson
            except ImportError:
                print("è¯·å…ˆå®‰è£…ijson: pip install ijson")
                return why_texts
            
            count = 0
            with open(why_path, 'r', encoding='utf-8') as f:
                # æµå¼è§£æJSONæ•°ç»„ä¸­çš„æ¯ä¸ªå¯¹è±¡
                parser = ijson.parse(f)
                current_item = {}
                current_key = None
                in_item = False
                for prefix, event, value in parser:
                    if prefix == 'item' and event == 'start_map':
                        in_item = True
                        current_item = {}
                    elif in_item and event == 'map_key':
                        current_key = value
                    elif in_item and event in ['string', 'number']:
                        if current_key:
                            current_item[current_key] = value
                    elif prefix == 'item' and event == 'end_map':
                        in_item = False
                        count += 1
                        # å¤„ç†å½“å‰é¡¹
                        prompt = current_item.get('prompt', '')
                        response = current_item.get('response', '')
                        if prompt and response:
                            text = f"æŒ‡ä»¤ï¼š{prompt}\nå›ç­”ï¼š{response}"
                            if len(text) > 4:
                                why_texts.append(text)
                        # æ¯å¤„ç†1000æ¡æ˜¾ç¤ºè¿›åº¦
                        if count % 1000 == 0:
                            print(f"å·²å¤„ç† {count} æ¡æ•°æ®ï¼Œå½“å‰æœ‰æ•ˆ: {len(why_texts)} æ¡")
                        current_item = {}
            print(f"âœ… Whyæ•°æ®æµå¼è¯»å–å®Œæˆ: æ€»å…±{count}æ¡ï¼Œæœ‰æ•ˆ{len(why_texts)}æ¡")
        except Exception as e:
            print(f"âŒ Whyæ•°æ®åŠ è½½é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        return why_texts
    def _load_json7_data(self, json7_path):
        """åŠ è½½ 7.json æ•°æ®ï¼ˆæ¯è¡Œä¸ºç‹¬ç«‹ JSON å¯¹è±¡ï¼‰"""
        texts = []
        try:
            with open(json7_path, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                # åˆ†å‰²ä¸ºå¤šè¡Œ JSON å¯¹è±¡ï¼ˆå®¹å¿æœ«å°¾é€—å·ï¼‰
                json_objects = [line.strip().rstrip(',') for line in content.split('},\n{')]
                # ä¿®å¤é¦–å°¾ç¼ºå¤±çš„å¤§æ‹¬å·
                if len(json_objects) == 1:
                    # æ•´ä¸ªæ–‡ä»¶æ˜¯ä¸€ä¸ªæ•°ç»„
                    data = json.loads(content)
                    for item in data:
                        if 'text' in item:
                            texts.append(item['text'])
                else:
                    # å¤šè¡Œç‹¬ç«‹ JSON å¯¹è±¡
                    if not content.startswith('{'):
                        json_objects[0] = '{' + json_objects[0]
                    if not content.endswith('}'):
                        json_objects[-1] = json_objects[-1] + '}'
                    for obj_str in json_objects:
                        try:
                            item = json.loads(obj_str)
                            if 'text' in item:
                                texts.append(item['text'])
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            print(f"7.jsonæ•°æ®åŠ è½½é”™è¯¯: {e}")
        return texts

    def _clean_texts(self, texts):
        """æ¸…æ´—æ–‡æœ¬æ•°æ®"""
        cleaned = []
        for text in texts:
            if not text or not isinstance(text, str):
                continue
            text = text.strip()
            if len(text) < 4 or len(text) > 2000:
                continue
            if any(bad in text for bad in ['äººæ°‘æ”¿åºœ','æ”¿æ²»å±€','æœ‰å…³éƒ¨é—¨','æ•™ç ”','æ•™å¸ˆ','å…šå‘˜','å…¥å…š','æ£€æŸ¥æœºå…³','ç›‘å¯Ÿæœºå…³','ç›‘ç£æœºå…³','ä¹¡æ‘æŒ¯å…´','ä¸­åŒ»è¯','å…šçš„å','ä¸­å›½ç‰¹è‰²','æœºå…³äººå‘˜','æ£€å¯Ÿæœºå…³','ä¸­å›½å¼','å…±åŒä½“','ä¸­åæ°‘æ—','å…šå§”','å‰¯éƒ¨','å…šå§”ä¹¦è®°','å…šä¸­å¤®','ç§˜ä¹¦é•¿','å…šç»„ç»‡','å…šæ ¡','å­¦ä¹ å¼ºå›½','æŠ—æ—¥','HTTP','HTML','^','æ”¯ä¹¦','cos','sin','å…¬å¼','åœ°æ–¹æ”¿åºœ','\\','\\\\','-----','C++','Python','Java','å›½åŠ¡é™¢','è´¨æ•°','$',',,','ã€‚ã€‚', 'http://', 'https://', 'Copyright','ä¸­å›½å…±äº§å…š','å“ˆé©¬æ–¯','å›½æ°‘å…š','ä¹Œå…‹å…°','å®¶å›½æƒ…æ€€','ä¹ è¿‘å¹³','å°æ¹¾æ˜¯ä¸­å›½','æ³•è½®åŠŸ','å›½å®¶æ”¿ç­–','å“ˆè¨å…‹æ–¯å¦','å…šçš„é¢†å¯¼','æ”¹é©å¼€æ”¾','ç¤¾ä¼šä¸»ä¹‰','ä¸€å›½ä¸¤åˆ¶','ä¸­å…±ä¸­å¤®','ä¸­å¤®é›†æƒ','å›½å®¶å®‰å…¨','æ°‘æ—å›¢ç»“','æ”¿æ²»åˆ¶åº¦']):
                continue
            chinese_chars = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
            if chinese_chars / len(text) < 0.3:
                continue
            cleaned.append(text)
        print(f"è¿‡æ»¤åä¿ç•™ {len(cleaned)} æ¡æ–‡æœ¬")
        return cleaned

    def _generate_backup_data(self):
        """ç”Ÿæˆå¤‡ç”¨è®­ç»ƒæ•°æ®"""
        backup_texts = [
            "ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºå»æ•£æ­¥ã€‚",
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ—¨åœ¨åˆ›é€ èƒ½å¤Ÿæ‰§è¡Œæ™ºèƒ½ä»»åŠ¡çš„æœºå™¨ã€‚",
            "ä¸­å›½çš„é¦–éƒ½æ˜¯åŒ—äº¬ï¼Œå®ƒæ˜¯ä¸€åº§å†å²æ‚ ä¹…çš„åŸå¸‚ã€‚",
            "å­¦ä¹ ç¼–ç¨‹éœ€è¦è€å¿ƒå’Œå®è·µï¼Œå¤šå†™ä»£ç æ‰èƒ½æé«˜æŠ€èƒ½ã€‚",
            "å¥åº·çš„ç”Ÿæ´»æ–¹å¼åŒ…æ‹¬å‡è¡¡é¥®é£Ÿã€é€‚é‡è¿åŠ¨å’Œå……è¶³ç¡çœ ã€‚",
            "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„é‡è¦åˆ†æ”¯ï¼Œå®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚",
            "æ·±åº¦å­¦ä¹ é€šè¿‡ç¥ç»ç½‘ç»œæ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ï¼Œå®ç°å¤æ‚æ¨¡å¼è¯†åˆ«ã€‚",
            "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
            "è®¡ç®—æœºè§†è§‰æŠ€æœ¯ä½¿æœºå™¨èƒ½å¤Ÿè¯†åˆ«å’Œç†è§£å›¾åƒå†…å®¹ã€‚",
            "å¼ºåŒ–å­¦ä¹ é€šè¿‡è¯•é”™æœºåˆ¶è®©æ™ºèƒ½ä½“å­¦ä¹ æœ€ä¼˜å†³ç­–ç­–ç•¥ã€‚",
        ] * 10
        return backup_texts

    def create_dataloaders(self, batch_size=16):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨ - åªè¿”å›è®­ç»ƒé›†"""
        train_texts, _ = self.load_datasets()
        random.shuffle(train_texts)
        train_dataset = TextDataset(train_texts, self.tokenizer, self.max_length)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            num_workers=0
        )
        print(f"âœ… DataLoaderåˆ›å»ºå®Œæˆï¼Œå…± {len(train_loader)} ä¸ªæ‰¹æ¬¡ã€‚")
        return train_loader, None
# ====================ã€æ›¿æ¢ç»“æŸã€‘====================

def causal_loss(logits, labels, ignore_index=0):
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    loss = F.cross_entropy(
        shift_logits.view(-1, shift_logits.size(-1)),
        shift_labels.view(-1),
        ignore_index=ignore_index
    )
    return loss
# ====================ã€æ ¸å¿ƒä¿®æ”¹ï¼šTrainer é›†æˆè°ƒåº¦å™¨ + weight decay åˆ†ç»„ + AMPã€‘====================
class Trainer:
    def __init__(self, model, train_loader, device, tokenizer, learning_rate=1.5e-4,
                 warmup_ratio=0.0, min_lr_ratio=0.1):
        self.model = model.to(device)
        self.train_loader = train_loader
        self.device = device
        self.tokenizer = tokenizer
        self.learning_rate = learning_rate
        self.warmup_ratio = warmup_ratio
        self.min_lr_ratio = min_lr_ratio
        # --- æ··åˆç²¾åº¦ ---
        self.use_amp = (device.type == 'cuda')
        self.scaler = torch.amp.GradScaler('cuda', enabled=self.use_amp)
        print(f"âœ… è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œæ··åˆç²¾åº¦è®­ç»ƒ: {'å¯ç”¨' if self.use_amp else 'ç¦ç”¨'}")
        # --- Weight decay åˆ†ç»„ ---
        no_decay = ["bias", "LayerNorm.weight", "RMSNorm.weight", "norm.weight"]
        optimizer_grouped_parameters = [
            {"params": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],
             "weight_decay": 0.01},
            {"params": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],
             "weight_decay": 0.0},
        ]
        self.optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, betas=(0.9, 0.98))
        # --- å›ºå®šæµ‹è¯•ç”¨ä¾‹ ---
        self.fixed_prompts = [
            "ä½ å¥½ã€‚", "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ", "è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„ç¬‘è¯", "ä½ æœ€å–œæ¬¢ä»€ä¹ˆæ°´æœï¼Ÿ"
        ]
    def _run_generation_test(self, step):
        print(f"\n--- è¿›è¡Œç”Ÿæˆæµ‹è¯•ï¼Œæ­¥éª¤ {step} ---")
        self.model.eval()
        for prompt in self.fixed_prompts:
            input_ids = self.tokenizer.encode(prompt)
            input_tensor = torch.tensor([input_ids], dtype=torch.long).to(self.device)
            generated_ids = self.model.generate(
                input_tensor,
                max_length=50,
                temperature=0.8,
                top_p=0.9,
                repetition_penalty=1.2
            )
            generated_text = self.tokenizer.decode(generated_ids[0].tolist())
            print(f"  Prompt    : {prompt}")
            print(f"  Generated : {generated_text}")
            print("-" * 20)
        self.model.train()
        print(f"--- ç”Ÿæˆæµ‹è¯•å®Œæˆï¼Œç»§ç»­è®­ç»ƒ ---\n")
    def save_checkpoint(self, epoch, lr_history=None, is_final=False):
        import gc
        print("æ­£åœ¨é‡Šæ”¾å†…å­˜å¹¶å‡†å¤‡ä¿å­˜æ¨¡å‹...")
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        config_dict = {
            'vocab_size': self.model.vocab_size,
            'hidden_size': self.model.hidden_size,
            'num_layers': self.model.num_layers,
            'num_heads': self.model.num_heads,
            'max_seq_len': self.model.max_seq_len,
            'num_experts': getattr(self.model, 'num_experts', 4),
            'window_size': getattr(self.model, 'window_size', 16),
        }
        checkpoint = {
            'model_state_dict': self.model.state_dict(),
            'epoch': epoch,
            'config': config_dict,
            'lr_history': lr_history or [],
        }
        if is_final:
            filename = "expert_gpt_model_finetuned_final1.pth"
        else:
            filename = f"expert_gpt_finetuned_epoch{epoch+1}.pth"
        torch.save(checkpoint, filename, _use_new_zipfile_serialization=False)
        print(f"âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜: {filename}")
    def train_epoch(self, epoch, scheduler, lr_history):
        self.model.train()
        total_loss = 0
        total_steps = len(self.train_loader)
        progress_bar = tqdm(self.train_loader, desc=f'Epoch {epoch+1}/{self.epochs} è®­ç»ƒä¸­', unit='batch')
        for batch_idx, batch in enumerate(progress_bar):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            self.optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type=self.device.type, dtype=torch.bfloat16, enabled=self.use_amp):
                logits = self.model(input_ids)
                loss = causal_loss(logits, labels)
            if torch.isnan(loss) or torch.isinf(loss):
                print(f"âš ï¸ æ£€æµ‹åˆ°Lossä¸ºNaN/Infï¼Œè·³è¿‡æœ¬æ¬¡æ›´æ–°ï¼")
                continue
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
            scheduler.step()
            current_lr = self.optimizer.param_groups[0]['lr']
            lr_history.append(current_lr)
            total_loss += loss.item()
            progress_bar.set_postfix({'loss': f'{total_loss / (progress_bar.n + 1):.4f}', 'lr': f'{current_lr:.2e}'})
            if (batch_idx + 1) % 400 == 0:
                self._run_generation_test(step=batch_idx + 1)
        avg_loss = total_loss / len(self.train_loader)
        print(f"âœ… Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
        return avg_loss
    def train(self, epochs):
        self.epochs = epochs
        total_steps = len(self.train_loader) * epochs
        warmup_steps = int(len(self.train_loader) * self.warmup_ratio)
        scheduler = CosineLRScheduler.get_cosine_schedule(
            self.optimizer, warmup_steps, total_steps, self.min_lr_ratio
        )
        lr_history = []
        print(f"ğŸš€ å¼€å§‹å¾®è°ƒè®­ç»ƒ ExpertGPTModelï¼Œå…± {epochs} ä¸ª Epoch...")
        for epoch in range(epochs):
            self.train_epoch(epoch, scheduler, lr_history)
        print("ğŸ‰ å¾®è°ƒè®­ç»ƒå®Œæˆï¼")
        self.save_checkpoint(epoch=epochs-1, lr_history=lr_history, is_final=True)
# ====================ã€ä¿®æ”¹ç»“æŸã€‘====================
def generate_text(model, tokenizer, prompt, max_length=50, device='cpu', temperature=0.8, top_p=0.9, repetition_penalty=1.2):
    model.eval()
    input_ids = tokenizer.encode(prompt)
    input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
    generated_ids = model.generate(
        input_tensor,
        max_length=max_length,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty
    )
    return tokenizer.decode(generated_ids[0].tolist())

def main():
    config = {
        'vocab_size': 21128, 'hidden_size': 768, 'num_layers': 12,
        'num_experts': 2, 'num_heads': 12, 'window_size': 32,
        'max_seq_len': 700, 'batch_size': 3, 'epochs': 1, 'learning_rate': 3e-5,  # å¾®è°ƒé€šå¸¸ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
        'warmup_ratio': 0.03, 'min_lr_ratio': 0.1  # æ–°å¢è°ƒåº¦å™¨å‚æ•°
    }
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ğŸŒ ä½¿ç”¨è®¾å¤‡: {device}")
    
    data_manager = DataManager()
    train_loader, _ = data_manager.create_dataloaders(batch_size=config['batch_size'])
    config['vocab_size'] = data_manager.tokenizer.vocab_size
    
    # åˆ›å»ºæ¨¡å‹
    print("ğŸ§© æ­£åœ¨åˆ›å»º ExpertGPTModel...")
    model = ExpertGPTModel(
        vocab_size=config['vocab_size'], hidden_size=config['hidden_size'],
        num_layers=config['num_layers'], num_experts=config['num_experts'],
        num_heads=config['num_heads'], window_size=config['window_size'],
        max_seq_len=config['max_seq_len']
    )
    model.lm_head.weight = model.token_embedding.weight
    
    # ====================ã€å…³é”®ï¼šåŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡è¿›è¡Œå¾®è°ƒã€‘====================
    pretrained_path = "./expert_gpt_model_final.pth"  # é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
    if os.path.exists(pretrained_path):
        print(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹æƒé‡: {pretrained_path}")
        checkpoint = torch.load(pretrained_path, map_location=device, weights_only=True)
        
        # æå–æ¨¡å‹çŠ¶æ€å­—å…¸
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        
        # å°è¯•åŠ è½½æƒé‡
        try:
            # strict=False å…è®¸éƒ¨åˆ†æƒé‡ä¸åŒ¹é…
            model.load_state_dict(state_dict, strict=False)
            print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼")
        except RuntimeError as e:
            print(f"âš ï¸ è­¦å‘Šï¼šéƒ¨åˆ†æƒé‡åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–: {e}")
            # å°è¯•éƒ¨åˆ†åŠ è½½
            model_dict = model.state_dict()
            # 1. filter out unnecessary keys
            pretrained_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.shape == model_dict[k].shape}
            # 2. overwrite entries in the existing state dict
            model_dict.update(pretrained_dict) 
            # 3. load the new state dict
            model.load_state_dict(model_dict)
            print(f"âœ… éƒ¨åˆ†æƒé‡åŠ è½½æˆåŠŸ ({len(pretrained_dict)}/{len(model_dict)} å±‚)")
    else:
        print(f"âŒ é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {pretrained_path}")
        print("âš ï¸ è­¦å‘Šï¼šå°†ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹")
    # ====================ã€åŠ è½½ç»“æŸã€‘====================
    
    # å‚æ•°ç»Ÿè®¡
    print("\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print("-" * 50)
    total_params = 0
    trainable_params = 0
    for name, module in model.named_modules():
        if isinstance(module, (nn.Linear, nn.Embedding)) or isinstance(module, nn.Parameter):
            module_params = sum(p.numel() for p in module.parameters())
            if module_params > 0:
                total_params += module_params
                trainable_params += module_params
                if "token_embedding" in name or "lm_head" in name or "expert" in name or "proj" in name:
                    print(f"  {name:30s}: {module_params:>10,} å‚æ•°")
    for name, param in model.named_parameters():
        if not param.requires_grad:
            total_params += param.numel()
    print("-" * 50)
    print(f"  ğŸ¯ æ€»è®¡å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  ğŸ“ˆ æ¨¡å‹æ€»å‚æ•°é‡: {total_params:,}")
    print(f"    â‰ˆ {total_params / 1e6:.2f}M å‚æ•°")
    print(f"    â‰ˆ {total_params / 1e9:.3f}B å‚æ•°")
    print("-" * 50)
    
    param_types = {}
    for name, param in model.named_parameters():
        param_type = name.split('.')[-2] if len(name.split('.')) >= 2 else 'other'
        if 'weight' in name or 'bias' in name:
            param_type = name.split('.')[-1]
        if param_type not in param_types:
            param_types[param_type] = 0
        param_types[param_type] += param.numel()
    
    for ptype, count in sorted(param_types.items(), key=lambda x: x[1], reverse=True):
        percentage = count / total_params * 100
        print(f"  {ptype:15s}: {count:>12,}  ({percentage:.1f}%)")
    
    print(f"\nğŸ—ï¸  æ¨¡å‹æ¶æ„:")
    print(f"  éšè—ç»´åº¦: {config['hidden_size']}")
    print(f"  å±‚æ•°: {config['num_layers']}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {config['num_heads']}")
    print(f"  ä¸“å®¶æ•°: {config['num_experts']}")
    print(f"  è¯æ±‡è¡¨å¤§å°: {config['vocab_size']:,}")
    print(f"  æœ€å¤§åºåˆ—é•¿åº¦: {config['max_seq_len']}")
    print(f"  å­¦ä¹ ç‡: {config['learning_rate']}")
    print(f"  å¾®è°ƒè½®æ•°: {config['epochs']}")
    
    # ====================ã€å…³é”®ï¼šTrainer åˆå§‹åŒ–å¢åŠ è°ƒåº¦å™¨å‚æ•°ã€‘====================
    trainer = Trainer(
        model, train_loader, device, data_manager.tokenizer,
        learning_rate=config['learning_rate'],
        warmup_ratio=config['warmup_ratio'],
        min_lr_ratio=config['min_lr_ratio']
    )
    trainer.train(epochs=config['epochs'])
    print("ğŸ’¾ å¾®è°ƒæ¨¡å‹å·²ä¿å­˜åˆ° expert_gpt_model_finetuned_final1.pth")
    
    print("\n--- ğŸ¤– è¿›å…¥æ¨ç†æ¨¡å¼ ---")
    prompt = "ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ"
    generated = generate_text(model, data_manager.tokenizer, prompt, device=device)
    print(f"Prompt: {prompt}")
    print(f"Generated: {generated}")
    
    prompt = "è¯·ç»™æˆ‘è®²ä¸€ä¸ªå…³äºäººå·¥æ™ºèƒ½çš„ç¬‘è¯"
    generated = generate_text(model, data_manager.tokenizer, prompt, device=device)
    print(f"Prompt: {prompt}")
    print(f"Generated: {generated}")

if __name__ == '__main__':
    seed = int(time.time() * 1000) % 2**32
    torch.manual_seed(seed)
    random.seed(seed)
    main()