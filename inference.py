#!/usr/bin/env python3
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
import os
import json
import random
import time
import sys
import threading
from tqdm import tqdm
from transformers import BertTokenizer
from collections import deque
import re

# ==================== æ¨¡å‹å®šä¹‰ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰====================
class RotaryPositionEmbedding(nn.Module):
    def __init__(self, dim, num_heads, max_seq_len=256, base_min=2000.0, base_max=100000.0):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads
        self.max_seq_len = max_seq_len
        self.current_seq_len_cached = 0
        num_head_groups = num_heads // 2
        group_bases = torch.logspace(
            start=math.log10(base_min),
            end=math.log10(base_max),
            steps=num_head_groups
        )
        base_list = [base for base in group_bases for _ in range(2)]
        self.base_list = torch.tensor(base_list)
        self.register_buffer("base_cache", self.base_list, persistent=True)
        self._update_freqs(seq_len=max_seq_len)
    
    def _update_freqs(self, seq_len, device='cpu'):
        if seq_len > self.max_seq_len:
            alpha = (seq_len / self.max_seq_len)
            current_bases = self.base_cache * (alpha ** (self.dim / (self.dim - 2)))
        else:
            current_bases = self.base_cache
        current_bases = current_bases.to(device)
        inv_freq_list = [1.0 / (base ** (torch.arange(0, self.dim, 2, device=device).float() / self.dim)) for base in current_bases]
        inv_freq = torch.stack(inv_freq_list)
        t = torch.arange(seq_len, device=device, dtype=inv_freq.dtype)
        freqs = torch.einsum('i,hj->hij', t, inv_freq)
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer('cos_cached', emb.cos(), persistent=False)
        self.register_buffer('sin_cached', emb.sin(), persistent=False)
        self.current_seq_len_cached = seq_len
    
    def forward(self, x, seq_len=None):
        if seq_len is None:
            seq_len = x.shape[-2]
        if self.cos_cached.device != x.device or seq_len > self.current_seq_len_cached:
            self._update_freqs(seq_len, device=x.device)
        cos = self.cos_cached[:, :seq_len, ...].unsqueeze(0)
        sin = self.sin_cached[:, :seq_len, ...].unsqueeze(0)
        x1, x2 = x[..., : self.dim // 2], x[..., self.dim // 2 :]
        rotated = torch.cat((-x2, x1), dim=-1)
        return (x * cos) + (rotated * sin)

class SingleAttentionExpert(nn.Module):
    def __init__(self, hidden_size, num_heads, window_size):
        super().__init__()
        assert hidden_size % num_heads == 0
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.window_size = window_size
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)
    
    def forward(self, x, attention_mask=None, rope=None):
        batch_size, seq_len, _ = x.shape
        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        
        if rope is not None:
            q = rope(q)
            k = rope(k)
        
        attn_output = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask, is_causal=False)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        return self.o_proj(attn_output)

class ParallelExpertAttention(nn.Module):
    def __init__(self, hidden_size=768, num_experts=2, num_heads=12, window_size=32, is_global=False):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.window_size = window_size
        self.is_global = is_global
        
        self.experts = nn.ModuleList(
            [SingleAttentionExpert(hidden_size, num_heads, window_size) for _ in range(num_experts)]
        )
        fused_dim = num_experts * hidden_size
        self.gate_proj = nn.Linear(fused_dim, hidden_size, bias=False)
        self.up_proj = nn.Linear(fused_dim, hidden_size, bias=False)
        self.dropout = nn.Dropout(0.1)
        self.register_buffer("sliding_window_mask", None, persistent=False)
        self.last_mask_len = 0

    def _get_mask(self, seq_len, device):
        if self.last_mask_len != seq_len or self.sliding_window_mask is None:
            if self.is_global:
                mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
            else:
                causal_mask = torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))
                dists = torch.arange(seq_len).unsqueeze(1) - torch.arange(seq_len).unsqueeze(0)
                sliding_window_mask = (dists >= 0) & (dists < self.window_size)
                mask = sliding_window_mask
            self.sliding_window_mask = mask.to(device)
            self.last_mask_len = seq_len
        return self.sliding_window_mask

    def forward(self, x, rope=None):
        batch_size, seq_len, _ = x.shape
        attention_mask = self._get_mask(seq_len, x.device)
        expert_outputs = []
        for expert in self.experts:
            output = expert(x, attention_mask=attention_mask, rope=rope)
            expert_outputs.append(output)
        fused_output = torch.cat(expert_outputs, dim=-1)
        gate = F.silu(self.gate_proj(fused_output))
        value = self.up_proj(fused_output)
        fused_result = self.dropout(gate * value)
        return fused_result

class GeGLUFeedForward(nn.Module):
    def __init__(self, hidden_size, expansion_ratio=3):
        super().__init__()
        self.hidden_size = hidden_size
        self.intermediate_size = int(hidden_size * expansion_ratio)
        self.gate_proj = nn.Linear(hidden_size, self.intermediate_size)
        self.up_proj = nn.Linear(hidden_size, self.intermediate_size)
        self.down_proj = nn.Linear(self.intermediate_size, hidden_size)
        self.dropout = nn.Dropout(0.1)
    
    def forward(self, x):
        x = F.silu(self.gate_proj(x)) * self.up_proj(x)
        x = self.down_proj(x)
        x = self.dropout(x)
        return x

class ExpertTransformerLayer(nn.Module):
    def __init__(self, hidden_size=768, num_experts=2, num_heads=12, window_size=32, is_global=False):
        super().__init__()
        self.self_attn = ParallelExpertAttention(
            hidden_size=hidden_size,
            num_experts=num_experts,
            num_heads=num_heads,
            window_size=window_size,
            is_global=is_global
        )
        self.ffn = GeGLUFeedForward(hidden_size)
        self.norm1 = nn.RMSNorm(hidden_size)
        self.norm2 = nn.RMSNorm(hidden_size)

    def forward(self, x, rope=None):
        attn_output = self.self_attn(self.norm1(x), rope=rope)
        x = x + attn_output
        ffn_output = self.ffn(self.norm2(x))
        x = x + ffn_output
        return x

class ExpertGPTModel(nn.Module):
    def __init__(self, vocab_size=21128, hidden_size=768, num_layers=12,
                 num_experts=2, num_heads=12, window_size=32, max_seq_len=256):
        super().__init__()
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_heads = num_heads
        self.max_seq_len = max_seq_len
        self.num_experts = num_experts
        self.window_size = window_size
        self.token_embedding = nn.Embedding(vocab_size, hidden_size)
        head_dim = hidden_size // num_heads
        self.rope = RotaryPositionEmbedding(head_dim, num_heads, max_seq_len=max_seq_len)
        
        global_layer_indices = {num_layers // 3, (num_layers * 2) // 3}
        layers = []
        for i in range(num_layers):
            is_global = i in global_layer_indices
            layers.append(
                ExpertTransformerLayer(
                    hidden_size=hidden_size,
                    num_experts=num_experts,
                    num_heads=num_heads,
                    window_size=window_size,
                    is_global=is_global
                )
            )
        self.layers = nn.ModuleList(layers)
        self.final_norm = nn.RMSNorm(hidden_size)
        self.lm_head = nn.Linear(hidden_size, vocab_size, bias=False)
    
    def forward(self, input_ids):
        batch_size, seq_len = input_ids.shape
        x = self.token_embedding(input_ids)
        for layer in self.layers:
            x = layer(x, rope=self.rope)
        x = self.final_norm(x)
        logits = self.lm_head(x)
        return logits

    def generate(self, input_ids, max_length=256, temperature=0.8, top_p=0.9, repetition_penalty=1.2):
        self.eval()
        generated = input_ids
        appeared_tokens = set(generated[0].tolist())
        with torch.no_grad():
            for _ in range(max_length):
                logits = self.forward(generated)[:, -1, :]
                if repetition_penalty != 1.0:
                    for token in appeared_tokens:
                        logits[0, token] /= repetition_penalty
                if temperature > 0:
                    logits = logits / temperature
                    probs = F.softmax(logits, dim=-1)
                    if top_p < 1.0:
                        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
                        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0
                        indices_to_remove = sorted_indices[sorted_indices_to_remove]
                        probs[..., indices_to_remove] = 0
                        if probs.sum() > 0:
                            probs = probs / probs.sum()
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(logits, dim=-1, keepdim=True)
                next_token_item = next_token.item()
                appeared_tokens.add(next_token_item)
                generated = torch.cat([generated, next_token], dim=1)
                if next_token.item() in [102, 0]:
                    break
        self.train()
        return generated

# ==================== å®ç”¨å·¥å…·å‡½æ•° ====================
def clear_screen():
    """è·¨å¹³å°æ¸…å±"""
    os.system('cls' if os.name == 'nt' else 'clear')

def typing_effect(text, delay=0.03, color_code="\033[94m"):
    """æ¨¡æ‹Ÿæ‰“å­—æ•ˆæœ"""
    print(color_code, end="", flush=True)
    for char in text:
        sys.stdout.write(char)
        sys.stdout.flush()
        time.sleep(delay)
    print("\033[0m", end="", flush=True)

def loading_animation(stop_event, message="æ€è€ƒä¸­"):
    """æ€è€ƒåŠ è½½åŠ¨ç”»"""
    animation = ["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]
    idx = 0
    while not stop_event.is_set():
        sys.stdout.write(f"\r\033[93m{message} {animation[idx % len(animation)]}\033[0m")
        sys.stdout.flush()
        idx += 1
        time.sleep(0.1)
    sys.stdout.write("\r" + " " * (len(message) + 10) + "\r")
    sys.stdout.flush()

def generate_text(model, tokenizer, prompt, max_length=256, temperature=0.8, top_p=0.9, repetition_penalty=1.2, device='cpu'):
    """ç”Ÿæˆæ–‡æœ¬å¹¶æ˜¾ç¤ºæ€è€ƒåŠ¨ç”»"""
    stop_event = threading.Event()
    animation_thread = threading.Thread(target=loading_animation, args=(stop_event, "æ¨¡å‹æ€è€ƒä¸­"))
    animation_thread.start()
    
    try:
        input_ids = tokenizer.encode(prompt, add_special_tokens=False)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(device)
        
        with torch.no_grad():
            generated_ids = model.generate(
                input_tensor,
                max_length=max_length,
                temperature=temperature,
                top_p=top_p,
                repetition_penalty=repetition_penalty
            )
        
        generated_text = tokenizer.decode(generated_ids[0].tolist())
        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™ç”Ÿæˆçš„å›ç­”
        response = generated_text[len(prompt):].strip()
        return response
    finally:
        stop_event.set()
        animation_thread.join()

# ==================== äº¤äº’å¼å¯¹è¯ç³»ç»Ÿ ====================
class ChatInterface:
    def __init__(self, model_path="expert_gpt_model_finetuned_final.pth"):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹åˆ° {self.device}...")
        
        # åŠ è½½tokenizer
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')
        print(f"ğŸ”¤ TokenizeråŠ è½½å®Œæˆ (è¯æ±‡è¡¨å¤§å°: {self.tokenizer.vocab_size})")
        
        # åŠ è½½æ¨¡å‹
        self.model = self.load_model(model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # åˆå§‹åŒ–å¯¹è¯å†å²
        self.history = deque(maxlen=10)  # ä¿ç•™æœ€è¿‘10è½®å¯¹è¯
        self.generation_params = {
            "temperature": 0.8,
            "top_p": 0.9,
            "repetition_penalty": 1.2,
            "max_length": 1000
        }
        
        # ä¸»é¢˜è®¾ç½®
        self.themes = {
            "default": ("\033[94m", "\033[92m"),  # ç”¨æˆ·è“ï¼ŒAIç»¿
            "dark": ("\033[96m", "\033[95m"),     # ç”¨æˆ·é’ï¼ŒAIç´«
            "retro": ("\033[93m", "\033[91m"),    # ç”¨æˆ·é»„ï¼ŒAIçº¢
            "nature": ("\033[92m", "\033[93m")    # ç”¨æˆ·ç»¿ï¼ŒAIé»„
        }
        self.current_theme = "default"
        self.user_color, self.ai_color = self.themes[self.current_theme]
        
        # æ¬¢è¿æ¶ˆæ¯
        self.show_welcome()
    
    def load_model(self, model_path):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # ä»checkpointåŠ è½½é…ç½®
        checkpoint = torch.load(model_path, map_location='cpu', weights_only=True)
        config = checkpoint['config']
        
        # åˆ›å»ºæ¨¡å‹
        model = ExpertGPTModel(
            vocab_size=config['vocab_size'],
            hidden_size=config['hidden_size'],
            num_layers=config['num_layers'],
            num_experts=config['num_experts'],
            num_heads=config['num_heads'],
            window_size=config['window_size'],
            max_seq_len=config['max_seq_len']
        )
        
        # åŠ è½½æƒé‡
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"ğŸ§  æ¨¡å‹åŠ è½½æˆåŠŸ! é…ç½®: {config['hidden_size']}ç»´, {config['num_layers']}å±‚, {config['num_experts']}ä¸“å®¶")
        return model
    
    def show_welcome(self):
        """æ˜¾ç¤ºæ¬¢è¿ç•Œé¢"""
        clear_screen()
        art = r"""
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘  ğŸ¤– æ¬¢è¿ä½¿ç”¨ ExpertGPT äº¤äº’å¼å¯¹è¯ç³»ç»Ÿ!                      â•‘
        â•‘                                                              â•‘
        â•‘  âœ¨ ç‰¹è‰²åŠŸèƒ½:                                                â•‘
        â•‘     â€¢ å®æ—¶æ€è€ƒåŠ¨ç”»ä¸æ‰“å­—æ•ˆæœ                                  â•‘
        â•‘     â€¢ åŠ¨æ€è°ƒæ•´ç”Ÿæˆå‚æ•° (æ¸©åº¦/top_p/é‡å¤æƒ©ç½š)                  â•‘
        â•‘     â€¢ å¤šä¸»é¢˜è§†è§‰åˆ‡æ¢                                          â•‘
        â•‘     â€¢ å¯¹è¯å†å²ç®¡ç†ä¸å¯¼å‡º                                      â•‘
        â•‘     â€¢ æŒ‡ä»¤å¾®è°ƒä¼˜åŒ–çš„ä¸­æ–‡å¯¹è¯èƒ½åŠ›                              â•‘
        â•‘                                                              â•‘
        â•‘  ğŸ® å¿«æ·æŒ‡ä»¤:                                                â•‘
        â•‘     /help   - æ˜¾ç¤ºå¸®åŠ©èœå•                                    â•‘
        â•‘     /params - è°ƒæ•´ç”Ÿæˆå‚æ•°                                    â•‘
        â•‘     /theme  - åˆ‡æ¢æ˜¾ç¤ºä¸»é¢˜                                    â•‘
        â•‘     /history- æŸ¥çœ‹å¯¹è¯å†å²                                    â•‘
        â•‘     /save   - ä¿å­˜å¯¹è¯åˆ°æ–‡ä»¶                                  â•‘
        â•‘     /clear  - æ¸…ç©ºå¯¹è¯å†å²                                    â•‘
        â•‘     /exit   - é€€å‡ºå¯¹è¯                                        â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print("\033[1;96m" + art + "\033[0m")
        typing_effect("ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ! è¯·è¾“å…¥æ‚¨çš„é—®é¢˜å¼€å§‹å¯¹è¯...", delay=0.02, color_code="\033[1;93m")
        print("\n" + "="*60)
    
    def show_help(self):
        """æ˜¾ç¤ºå¸®åŠ©èœå•"""
        help_text = """
        ğŸ“š å¸®åŠ©èœå•:
        
        ğŸšï¸  å‚æ•°è°ƒæ•´:
          /params temp=0.7 top_p=0.95 rep=1.5 max=80
          - temp: ç”Ÿæˆéšæœºæ€§ (0.1-2.0, é»˜è®¤0.8)
          - top_p: æ ¸é‡‡æ ·é˜ˆå€¼ (0.1-1.0, é»˜è®¤0.9)
          - rep: é‡å¤æƒ©ç½š (1.0-2.0, é»˜è®¤1.2)
          - max: æœ€å¤§ç”Ÿæˆé•¿åº¦ (10-200, é»˜è®¤60)
        
        ğŸ¨ ä¸»é¢˜åˆ‡æ¢:
          /theme [default|dark|retro|nature]
          - default: æ ‡å‡†è“ç»¿é…è‰²
          - dark: æ·±è‰²é’ç´«é…è‰²
          - retro: å¤å¤é»„çº¢é…è‰²
          - nature: è‡ªç„¶ç»¿é»„é…è‰²
        
        ğŸ“œ å†å²ç®¡ç†:
          /history - æŸ¥çœ‹æœ€è¿‘10è½®å¯¹è¯
          /save [æ–‡ä»¶å] - ä¿å­˜å¯¹è¯ (é»˜è®¤: chat_history.txt)
          /clear - æ¸…ç©ºå¯¹è¯å†å²
        
        âš¡ å…¶ä»–:
          /exit - é€€å‡ºå¯¹è¯
        """
        print("\033[1;95m" + help_text + "\033[0m")
    
    def adjust_params(self, command):
        """è°ƒæ•´ç”Ÿæˆå‚æ•°"""
        try:
            # è§£æå‘½ä»¤: /params temp=0.7 top_p=0.95 rep=1.5 max=80
            parts = command.split()[1:]
            for part in parts:
                key, value = part.split('=')
                key = key.strip()
                value = float(value.strip())
                
                if key in ["temp", "temperature"]:
                    if 0.1 <= value <= 2.0:
                        self.generation_params["temperature"] = value
                    else:
                        raise ValueError("æ¸©åº¦åº”åœ¨0.1-2.0ä¹‹é—´")
                elif key in ["top_p"]:
                    if 0.1 <= value <= 1.0:
                        self.generation_params["top_p"] = value
                    else:
                        raise ValueError("top_påº”åœ¨0.1-1.0ä¹‹é—´")
                elif key in ["rep", "repetition_penalty"]:
                    if 1.0 <= value <= 2.0:
                        self.generation_params["repetition_penalty"] = value
                    else:
                        raise ValueError("é‡å¤æƒ©ç½šåº”åœ¨1.0-2.0ä¹‹é—´")
                elif key in ["max", "max_length"]:
                    if 1 <= value <= 1000:
                        self.generation_params["max_length"] = int(value)
                    else:
                        raise ValueError("æœ€å¤§é•¿åº¦åº”åœ¨1-256ä¹‹é—´")
                else:
                    raise ValueError(f"æœªçŸ¥å‚æ•°: {key}")
            
            # æ˜¾ç¤ºæ›´æ–°åçš„å‚æ•°
            params_str = ", ".join([f"{k}={v}" for k, v in self.generation_params.items()])
            typing_effect(f"âœ… ç”Ÿæˆå‚æ•°å·²æ›´æ–°: {params_str}", color_code="\033[1;92m")
        except Exception as e:
            typing_effect(f"âŒ å‚æ•°è°ƒæ•´å¤±è´¥: {str(e)}", color_code="\033[1;91m")
    
    def change_theme(self, command):
        """åˆ‡æ¢æ˜¾ç¤ºä¸»é¢˜"""
        try:
            theme_name = command.split()[1] if len(command.split()) > 1 else "default"
            if theme_name in self.themes:
                self.current_theme = theme_name
                self.user_color, self.ai_color = self.themes[theme_name]
                typing_effect(f"ğŸ¨ å·²åˆ‡æ¢åˆ° {theme_name} ä¸»é¢˜", color_code="\033[1;93m")
            else:
                valid_themes = ", ".join(self.themes.keys())
                typing_effect(f"âš ï¸ æ— æ•ˆä¸»é¢˜. å¯ç”¨ä¸»é¢˜: {valid_themes}", color_code="\033[1;91m")
        except IndexError:
            typing_effect("ğŸ’¡ ç”¨æ³•: /theme [ä¸»é¢˜å]", color_code="\033[1;93m")
    
    def show_history(self):
        """æ˜¾ç¤ºå¯¹è¯å†å²"""
        if not self.history:
            typing_effect("ğŸ“­ å¯¹è¯å†å²ä¸ºç©º", color_code="\033[1;93m")
            return
        
        print("\n\033[1;94m" + "="*30 + " å¯¹è¯å†å² " + "="*30 + "\033[0m")
        for i, (user_msg, ai_msg) in enumerate(self.history, 1):
            print(f"\033[1;96m[{i}] ç”¨æˆ·:\033[0m {user_msg}")
            print(f"\033[1;92m[{i}] AI:\033[0m {ai_msg}")
            print("-"*65)
        print("\033[1;93mæç¤º: ä½¿ç”¨ /clear æ¸…ç©ºå†å², /save ä¿å­˜å†å²\033[0m")
    
    def save_history(self, command):
        """ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶"""
        filename = command.split()[1] if len(command.split()) > 1 else "chat_history.txt"
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"ExpertGPT å¯¹è¯è®°å½• - {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write("="*60 + "\n\n")
                for i, (user_msg, ai_msg) in enumerate(self.history, 1):
                    f.write(f"[{i}] ç”¨æˆ·: {user_msg}\n")
                    f.write(f"[{i}] AI: {ai_msg}\n")
                    f.write("-"*40 + "\n\n")
            
            typing_effect(f"ğŸ’¾ å¯¹è¯å†å²å·²ä¿å­˜åˆ° {filename}", color_code="\033[1;92m")
        except Exception as e:
            typing_effect(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}", color_code="\033[1;91m")
    
    def process_command(self, command):
        """å¤„ç†ç‰¹æ®Šå‘½ä»¤"""
        cmd = command.strip().lower()
        
        if cmd.startswith("/help"):
            self.show_help()
            return True
            
        elif cmd.startswith("/params"):
            self.adjust_params(command)
            return True
            
        elif cmd.startswith("/theme"):
            self.change_theme(command)
            return True
            
        elif cmd.startswith("/history"):
            self.show_history()
            return True
            
        elif cmd.startswith("/save"):
            self.save_history(command)
            return True
            
        elif cmd.startswith("/clear"):
            self.history.clear()
            typing_effect("ğŸ§¹ å¯¹è¯å†å²å·²æ¸…ç©º", color_code="\033[1;93m")
            return True
            
        elif cmd.startswith("/exit"):
            self.exit_chat()
            return False
            
        return False  # ä¸æ˜¯å‘½ä»¤ï¼Œç»§ç»­å¯¹è¯
    
    def exit_chat(self):
        """é€€å‡ºå¯¹è¯"""
        if self.history:
            print("\n\033[1;93m" + "="*30 + " å¯¹è¯æ€»ç»“ " + "="*30 + "\033[0m")
            # å°†dequeè½¬æ¢ä¸ºåˆ—è¡¨åå†åˆ‡ç‰‡
            for i, (user_msg, ai_msg) in enumerate(list(self.history)[-3:], 1):
                print(f"\033[1;96mæœ€å[{i}] ç”¨æˆ·:\033[0m {user_msg}")
                print(f"\033[1;92mæœ€å[{i}] AI:\033[0m {ai_msg}")
                print("-"*65)
            
            if input("\033[1;93mè¦ä¿å­˜å¯¹è¯å†å²å—? (y/n): \033[0m").lower().strip() == 'y':
                self.save_history("/save")
        
        goodbye_art = r"""
        â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
        â•‘                                                       â•‘
        â•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â•‘
        â•‘  â–ˆâ–ˆâ•”â•â•â•â•â• â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â•‘
        â•‘  â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•‘
        â•‘  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•— â•‘
        â•‘  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘ â•‘
        â•‘   â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•  â•šâ•â•â•â•â•â•   â•šâ•â•â•â•  â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â• â•‘
        â•‘                                                       â•‘
        â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        """
        print("\033[1;96m" + goodbye_art + "\033[0m")
        typing_effect("æ„Ÿè°¢ä½¿ç”¨ ExpertGPT! æ„¿æ™ºæ…§ä¸ä½ åŒåœ¨ ğŸŒŸ", delay=0.05, color_code="\033[1;95m")
        sys.exit(0)
    
    def run(self):
        """è¿è¡Œå¯¹è¯å¾ªç¯"""
        while True:
            try:
                # ç”¨æˆ·è¾“å…¥
                print("\n" + "="*60)
                user_input = input(f"\n{self.user_color}ğŸ‘¤ ä½ : \033[0m").strip()
                
                # å¤„ç†ç©ºè¾“å…¥
                if not user_input:
                    continue
                
                # å¤„ç†å‘½ä»¤
                if self.process_command(user_input):
                    continue
                
                # ç”Ÿæˆå“åº”
                response = generate_text(
                    self.model,
                    self.tokenizer,
                    user_input,
                    max_length=self.generation_params["max_length"],
                    temperature=self.generation_params["temperature"],
                    top_p=self.generation_params["top_p"],
                    repetition_penalty=self.generation_params["repetition_penalty"],
                    device=self.device
                )
                
                # æ˜¾ç¤ºAIå“åº” (å¸¦æ‰“å­—æ•ˆæœ)
                print(f"\n{self.ai_color}ğŸ¤– AI: ", end="", flush=True)
                typing_effect(response, delay=0.03, color_code=self.ai_color)
                
                # ä¿å­˜åˆ°å†å²
                self.history.append((user_input, response))
                
            except KeyboardInterrupt:
                self.exit_chat()
            except Exception as e:
                typing_effect(f"âŒ å‘ç”Ÿé”™è¯¯: {str(e)}", color_code="\033[1;91m")
                import traceback
                traceback.print_exc()

if __name__ == "__main__":
    try:
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶
        model_path = "expert_gpt_model_finetuned_final.pth"
        if not os.path.exists(model_path):
            print(f"\033[1;91mâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}\033[0m")
            print("\033[1;93mğŸ’¡ è¯·ç¡®ä¿è®­ç»ƒå¥½çš„æ¨¡å‹æ–‡ä»¶åœ¨å½“å‰ç›®å½•ä¸‹\033[0m")
            sys.exit(1)
        
        # å¯åŠ¨å¯¹è¯ç³»ç»Ÿ
        chat = ChatInterface(model_path)
        chat.run()
        
    except Exception as e:
        print(f"\033[1;91mâŒ å¯åŠ¨å¤±è´¥: {str(e)}\033[0m")
        import traceback
        traceback.print_exc()
        sys.exit(1)